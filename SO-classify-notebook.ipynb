{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack Overflow Topic Classification using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load csv files\n",
    "spark, ml, security = pd.read_csv('dataset/SO-Spark.csv'), pd.read_csv('dataset/SO-ML.csv'), pd.read_csv('dataset/SO-Security.csv')\n",
    "#Add label columns\n",
    "spark['Label'] = 'spark'\n",
    "ml['Label'] = 'ml'\n",
    "security['Label'] = 'security'\n",
    "\n",
    "# Keep only 'title' and 'label' columns\n",
    "spark_filtered = spark[['Title', 'Label']]\n",
    "ml_filtered = ml[['Title', 'Label']]\n",
    "security_filtered = security[['Title', 'Label']]\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([spark_filtered, ml_filtered, security_filtered], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Title   150000 non-null  object\n",
      " 1   Label   150000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Dataframe info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark       50000\n",
       "ml          50000\n",
       "security    50000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe target distribution\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "Steps:\n",
    "- Remove non-alphabetical characters\n",
    "- Convert to lowercase\n",
    "- Remove stopwords\n",
    "- Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create a custom transformer to preprocess text\n",
    "class Preprecessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def get_wordnet_pos(self, word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    def _preprocess(self, text):\n",
    "        # Remove non-alphabetic characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        # Lemmatize text with POS tagging\n",
    "        tokens = [self.lemmatizer.lemmatize(token, self.get_wordnet_pos(token)) for token in tokens]\n",
    "        # Join if the word isn't a blank space\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(self._preprocess)\n",
    "    \n",
    "preprocessor = Preprecessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_Title'] = preprocessor.transform(df['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Label</th>\n",
       "      <th>Clean_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Different rlike behavior in Spark 1.6 and Spar...</td>\n",
       "      <td>spark</td>\n",
       "      <td>different rlike behavior spark spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getting a column as concatenated column from a...</td>\n",
       "      <td>spark</td>\n",
       "      <td>get column concatenate column reference table ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write data using JDBC connection to Azure SQL ...</td>\n",
       "      <td>spark</td>\n",
       "      <td>write data use jdbc connection azure sql db sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Get value from external client database for a ...</td>\n",
       "      <td>spark</td>\n",
       "      <td>get value external client database column valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to setup Apache Spark to use local hard di...</td>\n",
       "      <td>spark</td>\n",
       "      <td>setup apache spark use local hard disk data fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>Getting the CurrentUserID from Websecurity dir...</td>\n",
       "      <td>security</td>\n",
       "      <td>get currentuserid websecurity directly login c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>How to name images so that other image names c...</td>\n",
       "      <td>security</td>\n",
       "      <td>name image image name cant guess easily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>Only the owner can delete his/her books?</td>\n",
       "      <td>security</td>\n",
       "      <td>owner delete hisher book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>Samsung Adb after a few days becomes un-author...</td>\n",
       "      <td>security</td>\n",
       "      <td>samsung adb day becomes unauthorized killserve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>self signed cert. The underlying connection wa...</td>\n",
       "      <td>security</td>\n",
       "      <td>self sign cert underlie connection close could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title     Label  \\\n",
       "0       Different rlike behavior in Spark 1.6 and Spar...     spark   \n",
       "1       Getting a column as concatenated column from a...     spark   \n",
       "2       Write data using JDBC connection to Azure SQL ...     spark   \n",
       "3       Get value from external client database for a ...     spark   \n",
       "4       How to setup Apache Spark to use local hard di...     spark   \n",
       "...                                                   ...       ...   \n",
       "149995  Getting the CurrentUserID from Websecurity dir...  security   \n",
       "149996  How to name images so that other image names c...  security   \n",
       "149997           Only the owner can delete his/her books?  security   \n",
       "149998  Samsung Adb after a few days becomes un-author...  security   \n",
       "149999  self signed cert. The underlying connection wa...  security   \n",
       "\n",
       "                                              Clean_Title  \n",
       "0                    different rlike behavior spark spark  \n",
       "1       get column concatenate column reference table ...  \n",
       "2       write data use jdbc connection azure sql db sc...  \n",
       "3       get value external client database column valu...  \n",
       "4       setup apache spark use local hard disk data fi...  \n",
       "...                                                   ...  \n",
       "149995  get currentuserid websecurity directly login c...  \n",
       "149996            name image image name cant guess easily  \n",
       "149997                           owner delete hisher book  \n",
       "149998  samsung adb day becomes unauthorized killserve...  \n",
       "149999  self sign cert underlie connection close could...  \n",
       "\n",
       "[150000 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show untruncated 10 rows\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export this dataframe to csv as SO-preprocessed.csv under datasets folder\n",
    "import os\n",
    "df.to_csv(os.path.join('dataset', 'SO-preprocessed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150000lines [00:01, 142526.50lines/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x7fa896e1dca0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def tokenize_iterator(data):\n",
    "    for title in data:\n",
    "        yield tokenizer(title)\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenize_iterator(df['Clean_Title']))\n",
    "# Show vocab\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalize the text data to feed into the model\n",
    "def numericalize(text, tokenizer, vocab):\n",
    "    tokens = tokenizer(text)\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "df['Numericalized_Title'] = df['Clean_Title'].apply(lambda x: numericalize(x, tokenizer, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Label</th>\n",
       "      <th>Clean_Title</th>\n",
       "      <th>Numericalized_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Different rlike behavior in Spark 1.6 and Spar...</td>\n",
       "      <td>spark</td>\n",
       "      <td>different rlike behavior spark spark</td>\n",
       "      <td>[52, 3458, 821, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Getting a column as concatenated column from a...</td>\n",
       "      <td>spark</td>\n",
       "      <td>get column concatenate column reference table ...</td>\n",
       "      <td>[13, 9, 822, 9, 684, 65, 1701, 205, 45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write data using JDBC connection to Azure SQL ...</td>\n",
       "      <td>spark</td>\n",
       "      <td>write data use jdbc connection azure sql db sc...</td>\n",
       "      <td>[67, 4, 3, 467, 170, 176, 29, 444, 17, 39, 275...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Get value from external client database for a ...</td>\n",
       "      <td>spark</td>\n",
       "      <td>get value external client database column valu...</td>\n",
       "      <td>[13, 12, 320, 116, 99, 9, 12, 48, 2, 213]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to setup Apache Spark to use local hard di...</td>\n",
       "      <td>spark</td>\n",
       "      <td>setup apache spark use local hard disk data fi...</td>\n",
       "      <td>[644, 28, 2, 3, 180, 1322, 840, 4, 299, 1439, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>Getting the CurrentUserID from Websecurity dir...</td>\n",
       "      <td>security</td>\n",
       "      <td>get currentuserid websecurity directly login c...</td>\n",
       "      <td>[13, 19161, 2448, 761, 100, 7303]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>How to name images so that other image names c...</td>\n",
       "      <td>security</td>\n",
       "      <td>name image image name cant guess easily</td>\n",
       "      <td>[98, 44, 44, 98, 189, 2238, 2460]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>Only the owner can delete his/her books?</td>\n",
       "      <td>security</td>\n",
       "      <td>owner delete hisher book</td>\n",
       "      <td>[1639, 478, 11792, 2132]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>Samsung Adb after a few days becomes un-author...</td>\n",
       "      <td>security</td>\n",
       "      <td>samsung adb day becomes unauthorized killserve...</td>\n",
       "      <td>[9716, 5360, 828, 2073, 1189, 26007, 1074, 38]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>self signed cert. The underlying connection wa...</td>\n",
       "      <td>security</td>\n",
       "      <td>self sign cert underlie connection close could...</td>\n",
       "      <td>[992, 392, 1379, 2318, 170, 997, 289, 1693, 49...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title     Label  \\\n",
       "0       Different rlike behavior in Spark 1.6 and Spar...     spark   \n",
       "1       Getting a column as concatenated column from a...     spark   \n",
       "2       Write data using JDBC connection to Azure SQL ...     spark   \n",
       "3       Get value from external client database for a ...     spark   \n",
       "4       How to setup Apache Spark to use local hard di...     spark   \n",
       "...                                                   ...       ...   \n",
       "149995  Getting the CurrentUserID from Websecurity dir...  security   \n",
       "149996  How to name images so that other image names c...  security   \n",
       "149997           Only the owner can delete his/her books?  security   \n",
       "149998  Samsung Adb after a few days becomes un-author...  security   \n",
       "149999  self signed cert. The underlying connection wa...  security   \n",
       "\n",
       "                                              Clean_Title  \\\n",
       "0                    different rlike behavior spark spark   \n",
       "1       get column concatenate column reference table ...   \n",
       "2       write data use jdbc connection azure sql db sc...   \n",
       "3       get value external client database column valu...   \n",
       "4       setup apache spark use local hard disk data fi...   \n",
       "...                                                   ...   \n",
       "149995  get currentuserid websecurity directly login c...   \n",
       "149996            name image image name cant guess easily   \n",
       "149997                           owner delete hisher book   \n",
       "149998  samsung adb day becomes unauthorized killserve...   \n",
       "149999  self sign cert underlie connection close could...   \n",
       "\n",
       "                                      Numericalized_Title  \n",
       "0                                   [52, 3458, 821, 2, 2]  \n",
       "1                 [13, 9, 822, 9, 684, 65, 1701, 205, 45]  \n",
       "2       [67, 4, 3, 467, 170, 176, 29, 444, 17, 39, 275...  \n",
       "3               [13, 12, 320, 116, 99, 9, 12, 48, 2, 213]  \n",
       "4       [644, 28, 2, 3, 180, 1322, 840, 4, 299, 1439, ...  \n",
       "...                                                   ...  \n",
       "149995                  [13, 19161, 2448, 761, 100, 7303]  \n",
       "149996                  [98, 44, 44, 98, 189, 2238, 2460]  \n",
       "149997                           [1639, 478, 11792, 2132]  \n",
       "149998     [9716, 5360, 828, 2073, 1189, 26007, 1074, 38]  \n",
       "149999  [992, 392, 1379, 2318, 170, 997, 289, 1693, 49...  \n",
       "\n",
       "[150000 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Label'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StackOverflowDataset(Dataset):\n",
    "    def __init__(self, df, label_to_idx):\n",
    "        self.df = df\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.df.iloc[idx]['Numericalized_Title'], dtype=torch.long)\n",
    "        label = torch.tensor(self.label_to_idx[self.df.iloc[idx]['Label']], dtype=torch.long)\n",
    "        return text, label\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    text_lens = [len(text) for text in texts]\n",
    "    max_len = max(text_lens)\n",
    "    padded_texts = [torch.cat([text, torch.tensor([vocab['<pad>']] * (max_len - len(text)), dtype=torch.long)]) for text in texts]\n",
    "    return torch.stack(padded_texts), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "label_to_idx = {'spark': 0, 'ml': 1, 'security': 2}\n",
    "\n",
    "train_dataset = StackOverflowDataset(train_df, label_to_idx)\n",
    "val_dataset = StackOverflowDataset(val_df, label_to_idx)\n",
    "test_dataset = StackOverflowDataset(test_df, label_to_idx)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        pooled = self.dropout(torch.mean(lstm_out, dim=1))\n",
    "        logits = self.fc(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_classes = 3\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.2869 | Val Loss: 0.1803\n",
      "Epoch 2/10 | Train Loss: 0.1498 | Val Loss: 0.1540\n",
      "Epoch 3/10 | Train Loss: 0.1075 | Val Loss: 0.1562\n",
      "Epoch 4/10 | Train Loss: 0.0789 | Val Loss: 0.1716\n",
      "Epoch 5/10 | Train Loss: 0.0584 | Val Loss: 0.1873\n",
      "Epoch 6/10 | Train Loss: 0.0445 | Val Loss: 0.1965\n",
      "Epoch 7/10 | Train Loss: 0.0353 | Val Loss: 0.2249\n",
      "Epoch 8/10 | Train Loss: 0.0279 | Val Loss: 0.2235\n",
      "Epoch 9/10 | Train Loss: 0.0238 | Val Loss: 0.2468\n",
      "Epoch 10/10 | Train Loss: 0.0207 | Val Loss: 0.2798\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for texts, labels in train_dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(texts)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validate the model\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            logits = model(texts)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       spark       0.94      0.94      0.94     10000\n",
      "          ml       0.94      0.94      0.94     10000\n",
      "    security       0.96      0.96      0.96     10000\n",
      "\n",
      "    accuracy                           0.94     30000\n",
      "   macro avg       0.94      0.94      0.94     30000\n",
      "weighted avg       0.94      0.94      0.94     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        logits = model(texts)\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        true_labels.extend(labels.tolist())\n",
    "        pred_labels.extend(predictions.tolist())\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, target_names=label_to_idx.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9992f0915ac1c0371431034fe518ae802434a0f09f5f0bda48ab0a5bf0e52c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
